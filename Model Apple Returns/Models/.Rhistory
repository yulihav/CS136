market <- read.csv(file.choose())
market$Date <- as.Date(market$Date) # format date
head(market)
# plot Apple stock and returns, log returns, and volatily scaled returns
ndays <- nrow(market)
AAPL <- market$AAPL
vol <- market$VIX
# normal returns
ret.AAPL <- (AAPL[2:ndays]-AAPL[1:(ndays-1)])/AAPL[1:(ndays-1)]
market <- read.csv(file.choose())
market$Date <- as.Date(market$Date) # format date
head(market)
# plot Apple stock and returns, log returns, and volatily scaled returns
ndays <- nrow(market)
AAPL <- market$AAPL
vol <- market$VIX
# normal returns
ret.AAPL <- (AAPL[2:ndays]-AAPL[1:(ndays-1)])/AAPL[1:(ndays-1)]
summary(market)
summary(market)
nreps<-2e3 # number of replications
ntot<-nrow(market)# total number of observations
ntrain<-2700# size of training set
ntest<-ntot-ntrain# size of test set
sse1<-rep(NA, nreps)# sum-of-square errors for each CV replication
sse2<-rep(NA, nreps)
Lambda<-rep(NA, nreps)# likelihod ratio statistic for each replication
sse2<-rep(NA, nreps)
ntot<-nrow(market)# total number of observations
M1<-Mstep.4int
M2<-Mstep.4int2
M1<-Mstep.4int
M2<-Mstep.4int2
nreps<-2e3 # number of replications
ntot<-nrow(market)# total number of observations
ntrain<-2700# size of training set
ntest<-ntot-ntrain# size of test set
sse1<-rep(NA, nreps)# sum-of-square errors for each CV replication
sse2<-rep(NA, nreps)
Lambda<-rep(NA, nreps)# likelihod ratio statistic for each replication
system.time({
for(ii in 1:nreps) {
if(ii%%400 == 0)message("ii = ", ii)
# randomly select training observations
train.ind<-sample(ntot, ntrain)# training observations
#the faster R way
M1.cv<-update(M1, subset = train.ind)
M2.cv<-update(M2, subset = train.ind)
# testing residuals for both models
# that is, testing data - predictions with training parameters
M1.res<-market$AAPL[-train.ind] - predict(M1.cv, newdata = market[-train.ind,])
M2.res<-market$AAPL[-train.ind] - predict(M2.cv, newdata = market[-train.ind,])
# total sum of square errors
sse1[ii]<-sum((M1.res)^2)
sse2[ii]<-sum((M2.res)^2)
# testing likelihood ratio
M1.sigma<-sqrt(sum(resid(M1.cv)^2)/ntrain)# MLE of sigma
M2.sigma<-sqrt(sum(resid(M2.cv)^2)/ntrain)
Lambda[ii]<-sum(dnorm(M1.res, mean = 0, sd = M1.sigma, log =TRUE))
Lambda[ii]<-Lambda[ii] - sum(dnorm(M2.res, mean = 0, sd = M2.sigma, log =TRUE))
}})
}})
M1.res<-market$AAPL[-train.ind] - predict(M1.cv, newdata = market[-train.ind,])
M1.res<-market$AAPL[-train.ind] - predict(M1.cv, newdata = market[-train.ind,])
M1.res<-market$AAPL[-train.ind] - predict(M1.cv, newdata = market[-train.ind,])
M1.res<-market$AAPL - predict(M1.cv, newdata = market)
M1.res <- market$AAPL - predict(M1.cv, newdata = market)
market$AAPL
predict(M1.cv, newdata = market)
predict(M1,cv,data=market)
predict(M1.cv,data=market)
market$AAPL - predict(M1.cv,data=market)
market$AAPL[-train.ind] - predict(M1.cv,data=market[-train.ind,])
market$AAPL[-train.ind] - predict(M1.cv,newdata=market[-train.ind,])
system.time({
for(ii in 1:nreps) {
if(ii%%400 == 0)message("ii = ", ii)
# randomly select training observations
train.ind<-sample(ntot, ntrain)# training observations
#the faster R way
M1.cv<-update(M1, subset = train.ind)
M2.cv<-update(M2, subset = train.ind)
# testing residuals for both models
# that is, testing data - predictions with training parameters
M1.res<-market$AAPL[-train.ind] - predict(M1.cv, data = market[-train.ind,])
M2.res<-market$AAPL[-train.ind] - predict(M2.cv, data = market[-train.ind,])
# total sum of square errors
sse1[ii]<-sum((M1.res)^2)
sse2[ii]<-sum((M2.res)^2)
# testing likelihood ratio
M1.sigma<-sqrt(sum(resid(M1.cv)^2)/ntrain)# MLE of sigma
M2.sigma<-sqrt(sum(resid(M2.cv)^2)/ntrain)
Lambda[ii]<-sum(dnorm(M1.res, mean = 0, sd = M1.sigma, log =TRUE))
Lambda[ii]<-Lambda[ii] - sum(dnorm(M2.res, mean = 0, sd = M2.sigma, log =TRUE))
}})
warnings()
c(SSE1 = mean(sse1), SSE2 = mean(sse2))# favors M2
mean(Lambda)# log(Lambda) < 0 which favors M2
par(mar = c(3, 6, 1, 1))
Mnames<-expression(Mstep, Mstep.reduced)
boxplot(x = list(press1^2, press2^2), names = Mnames,
ylab = expression(PRESS[i]^2), col = c("yellow","orange"))
hist(Lambda, breaks = 50, freq =FALSE, xlab = expression(Lambda^{test}),
main ="", cex = .7)abline(v = mean(Lambda), col ="red")# average value
boxplot(x = list(press1^2, press2^2), names = Mnames,
ylab = expression(PRESS[i]^2), col = c("yellow","orange"))
hist(Lambda, breaks = 50, freq =FALSE, xlab = expression(Lambda^{test}),
main ="", cex = .7)abline(v = mean(Lambda), col ="red")# average value
hist(Lambda, breaks = 50, freq =FALSE, xlab = expression(Lambda^{test}),
main ="", cex = .7)
abline(v = mean(Lambda), col ="red")# average value
par(mfrow = c(1,2))
par(mar = c(4.5, 4.5, .1, .1))
boxplot(x = list(sse1, sse2), names = Mnames, cex = .7,
ylab = expression(SS[err]^{test}), col = c("yellow","orange"))
hist(Lambda, breaks = 50, freq =FALSE, xlab = expression(Lambda^{test}),
main ="", cex = .7)
abline(v = mean(Lambda), col ="red")# average value
# let's look at the Akaike Information Criterion for these models
AIC.M1 <- AIC(Mstep.4int)
AIC.M2 <- AIC(Mstep.4int2)
c(AIC1 = AIC.M1, AIC2 = AIC.M2)
##########################################################################################
#PRESS statistics
press1<-resid(Mstep.4int)/(1-hatvalues(Mstep.4int))# M1
press2<-resid(Mstep.4int2)/(1-hatvalues(Mstep.4int2))# M2
c(PRESS1 = sum(press1^2), PRESS2 = sum(press2^2))# favors M2
# plot PRESS statistics
par(mar = c(3, 6, 1, 1))
Mnames<-expression(Mstep, Mstep.reduced)
boxplot(x = list(press1^2, press2^2), names = Mnames,
ylab = expression(PRESS[i]^2), col = c("yellow","orange"))
##########################################################################################
#Cross-Validation
M1<-Mstep.4int
M2<-Mstep.4int2
nreps<-2e3 # number of replications
ntot<-nrow(market)# total number of observations
ntrain<-2700# size of training set
ntest<-ntot-ntrain# size of test set
sse1<-rep(NA, nreps)# sum-of-square errors for each CV replication
sse2<-rep(NA, nreps)
Lambda<-rep(NA, nreps)# likelihod ratio statistic for each replication
system.time({
for(ii in 1:nreps) {
if(ii%%400 == 0)message("ii = ", ii)
# randomly select training observations
train.ind<-sample(ntot, ntrain)# training observations
#the faster R way
M1.cv<-update(M1, subset = train.ind)
M2.cv<-update(M2, subset = train.ind)
# testing residuals for both models
# that is, testing data - predictions with training parameters
M1.res<-market$AAPL[-train.ind] - predict(M1.cv, data = market[-train.ind,]) #didn;t work when i wrote newdata instead of data???
M2.res<-market$AAPL[-train.ind] - predict(M2.cv, data = market[-train.ind,])
# total sum of square errors
sse1[ii]<-sum((M1.res)^2)
sse2[ii]<-sum((M2.res)^2)
# testing likelihood ratio
M1.sigma<-sqrt(sum(resid(M1.cv)^2)/ntrain)# MLE of sigma
M2.sigma<-sqrt(sum(resid(M2.cv)^2)/ntrain)
Lambda[ii]<-sum(dnorm(M1.res, mean = 0, sd = M1.sigma, log =TRUE))
Lambda[ii]<-Lambda[ii] - sum(dnorm(M2.res, mean = 0, sd = M2.sigma, log =TRUE))
}})
c(SSE1 = mean(sse1), SSE2 = mean(sse2))# favors M2
mean(Lambda)  # log(Lambda) < 0 which favors M2
# plot cross-validation SSE and Lambda
par(mfrow = c(1,2))
par(mar = c(4.5, 4.5, .1, .1))
boxplot(x = list(sse1, sse2), names = Mnames, cex = .7,
ylab = expression(SS[err]^{test}), col = c("yellow","orange"))
hist(Lambda, breaks = 50, freq =FALSE, xlab = expression(Lambda^{test}),
main ="", cex = .7)
abline(v = mean(Lambda), col ="red")# average value
# format dataset to get tomorrow's returns and data up to today, with
# 'log' and 'scaled' indicating whether the values returned should be
# transformed with logarithms and scaled respectively
ret.format.Xy <- function(ret.name, cov.names, p, data,
log = FALSE, scale = FALSE) {
n <- nrow(data)
data2 <- matrix(NA, n-p-1, length(cov.names)*(p+1) + 1)
data2 <- as.data.frame(data2)
# format design matrix to produce past dependence
AR.format.X <- function(x, p) {
p <- p+1
n <- length(x)
X <- matrix(NA, n-p+1, p)
for(ii in 1:p) {
X[,ii] <- x[1:(n-p+1)+(p-ii)]
}
X
}
# covariates
for(ii in 1:length(cov.names)) {
cind <- 1 + (p+1)*(ii-1)+1:(p+1) # column indices
data2[,cind] <- AR.format.X(data[-n,cov.names[ii]], p = p)
names(data2)[cind] <- paste0(cov.names[ii], 0:p)
}
# response
data2[,1] <- exp(diff(log(data[(p+1):nrow(data),ret.name])))-1
names(data2)[1] <- paste0("ret.", ret.name)
# scale and transform response if desired
if (log) {
data2[,1] <- data2[,1] + 1
data2 <- log(data2)
}
if (scale) {
data2 <- data2/sqrt(vol[p:(n-p-1)])
}
# add dates
data2[,(ncol(data2)+1)] <- AR.format.X(dates[2:n], p = p)[,1]
names(data2)[ncol(data2)] <- 'Date'
data2
}
# Preparation: getting and organizing data, general plots #####
# Load data
market <- read.csv(file.choose())
market$Date <- as.Date(market$Date) # format date
ndays <- nrow(market)
AAPL <- market$AAPL
vol <- market$VIX
dates <- market$Date
ret.name <- 'AAPL'
cov.names <- c('AAPL', 'SPX', 'VIX', 'SPGSCITR', 'BNDGLB', 'EEM')
cov2.names<- colnames(mdata3scl)
mdata3scl <- ret.format.Xy(ret.name = ret.name, cov.names = cov.names,
p = 2, data = market, scale = TRUE)
cov2.names<- colnames(mdata3scl)
M1intscl <- readRDS(file.choose())
setwd("~/Desktop/331 Project/NewFiles")
M1intscl <- readRDS(file.choose())
M3intscl <- readRDS(file.choose())
N1<-M1intscl
Msum<- summary(N1)
# residuals vs predicted values
y.hat <- predict(N1) # predicted values
sigma.hat <- Msum$sigma
res <- resid(N1) # original residuals$
stan.res <- res/sigma.hat # standardized residuals
# compute leverages
X <- model.matrix(N1)
H <- X %*% solve(crossprod(X), t(X)) # HAT matrix
head(diag(H))
h <- hatvalues(N1) # the R way
range(h - diag(H))
# studentized residuals
stud.res <- stan.res/sqrt(1-h)
# PRESS residuals
press <- res/(1-h)
# DFFITS residuals
dfts <- dffits(N1) # the R way
# standardize each of these such that they are identical at the average leverage value
p <- length(coef(N1))
n <- nobs(N1)
hbar <- p/n # average leverage
stud.res <- stud.res*sqrt(1-hbar) # at h = hbar, stud.res = stan.res
press <- press*(1-hbar)/sigma.hat # at h = hbar, press = stan.res
dfts <- dfts*(1-hbar)/sqrt(hbar) # at h = hbar, dfts = stan.res
# plot all residuals
par(mfrow = c(1,2), mar = c(4,4,.1,.1))
# against predicted values
cex <- .8
plot(y.hat, rep(0, length(y.hat)), type = "n", # empty plot to get the axis range
ylim = range(stan.res, stud.res, press, dfts), cex.axis = cex,
xlab = "Predicted Values", ylab = "Residuals")
# dotted line connecting each observations residuals for better visibility
segments(x0 = y.hat,
y0 = pmin(stud.res, dfts),
y1 = pmax(stud.res,dfts),
lty = 2)
#points(y.hat, stan.res, pch = 21, bg = "black", cex = cex)
points(y.hat, stud.res, pch = 21, bg = "blue", cex = cex)
#points(y.hat, press, pch = 21, bg = "red", cex = cex)
points(y.hat, dfts, pch = 21, bg = "red", cex = cex)
# against leverages
plot(h, rep(0, length(y.hat)), type = "n", cex.axis = cex,
ylim = range(stud.res, dfts),
xlab = "Leverages", ylab = "Residuals")
segments(x0 = h,
y0 = pmin(stud.res, dfts),
y1 = pmax( stud.res,dfts),
lty = 2)
#points(h, stan.res, pch = 21, bg = "black", cex = cex)
points(h, stud.res, pch = 21, bg = "blue", cex = cex)
#points(h, press, pch = 21, bg = "red", cex = cex)
points(h, dfts, pch = 21, bg = "red", cex = cex)
abline(v = hbar, col = "grey60", lty = 2)
legend("topright", legend = c("Studentized", "DFFITS"),
pch = 21, pt.bg = c("blue", "red"), title = "Residual Type:",
cex = cex, pt.cex = cex)
# standardize each of these such that they are identical at the average leverage value
p <- length(coef(N1))
n <- nobs(N1)
hbar <- p/n # average leverage
stud.res <- stud.res*sqrt(1-hbar) # at h = hbar, stud.res = stan.res
dfts <- dfts*(1-hbar)/sqrt(hbar) # at h = hbar, dfts = stan.res
# plot all residuals
par(mfrow = c(1,2), mar = c(4,4,.1,.1))
# against predicted values
cex <- .8
plot(y.hat, rep(0, length(y.hat)), type = "n", # empty plot to get the axis range
ylim = range(stan.res, stud.res, press, dfts), cex.axis = cex,
xlab = "Predicted Values", ylab = "Residuals")
# dotted line connecting each observations residuals for better visibility
segments(x0 = y.hat,
y0 = pmin(stud.res, dfts),
y1 = pmax(stud.res,dfts),
lty = 2)
#points(y.hat, stan.res, pch = 21, bg = "black", cex = cex)
points(y.hat, stud.res, pch = 21, bg = "blue", cex = cex)
#points(y.hat, press, pch = 21, bg = "red", cex = cex)
points(y.hat, dfts, pch = 21, bg = "red", cex = cex)
# against leverages
plot(h, rep(0, length(y.hat)), type = "n", cex.axis = cex,
ylim = range(stud.res, dfts),
xlab = "Leverages", ylab = "Residuals")
segments(x0 = h,
y0 = pmin(stud.res, dfts),
y1 = pmax( stud.res,dfts),
lty = 2)
#points(h, stan.res, pch = 21, bg = "black", cex = cex)
points(h, stud.res, pch = 21, bg = "blue", cex = cex)
#points(h, press, pch = 21, bg = "red", cex = cex)
points(h, dfts, pch = 21, bg = "red", cex = cex)
abline(v = hbar, col = "grey60", lty = 2)
legend("topright", legend = c("Studentized", "DFFITS"),
pch = 21, pt.bg = c("blue", "red"), title = "Residual Type:",
cex = cex, pt.cex = cex)
# cookâ€™s distance vs. leverage
D <- cooks.distance(N1)
# flag some of the points
infl.ind <- which.max(D) # top influence point
lev.ind <- h > 2*hbar # leverage more than 2x the average
clrs <- rep("black", len = n)
clrs[lev.ind] <- "blue"
clrs[infl.ind] <- "red"
par(mfrow = c(1,1), mar = c(4,4,1,1))
cex <- .8
plot(h, D, xlab = "Leverage", ylab = "Cookâ€™s Influence Measure",
pch = 21, bg = clrs, cex = cex, cex.axis = cex)
p <- length(coef(N1))
n <- nrow(market)
hbar <- p/n # average leverage
abline(v = 2*hbar, col = "grey60", lty = 2) # 2x average leverage
legend("topleft", legend = c("High Leverage", "High Influence"), pch = 21,
pt.bg = c("blue", "red"), cex = cex, pt.cex = cex)
mdata1scl <- ret.format.Xy(ret.name = ret.name, cov.names = cov.names,
p = 0, data = market, scale = TRUE)
cov1.names<- colnames(mdata1scl)
mdata3scl <- ret.format.Xy(ret.name = ret.name, cov.names = cov.names,
p = 2, data = market, scale = TRUE)
cov3.names<- colnames(mdata3scl)
N1<-M1intscl
summary(mdata1scl)
# predictions with omitted values
omit.ind <- c(infl.ind, # most influential
lev.ind) # highest leverage
names(omit.ind) <- c("infl", "lev")
yobs <- mdata1scl[,"ret.AAPL"] # observed values
yfitf <- predict(N1) # fitted values
par(mfrow = c(2,2))
par(mar = c(4,4,0,0)+.1)
cex <- .8
clrs2 <- c("red", "blue")
yobs <- mdata1scl[,"ret.AAPL"] # observed values
yfitf <- predict(N1) # fitted values
par(mfrow = c(2,2))
par(mar = c(4,4,0,0)+.1)
cex <- .8
clrs2 <- c("red", "blue")
for(jj in 1:length(omit.ind)) {
# model with omitted observation
N1.o <- update(N1, data = mdata1scl-omit.ind[jj])
yfito <- predict(N1.o, newdata = mdata1scl) # fitted values at all observations
length(yfitf)
length(xobs)
for(ii in cov1.names) {
# response vs. each covariate
xobs <- mdata1scl[,ii] # covariate
ylim <- range(yobs, yfitf, yfito) # y-range of plot
# observed
plot(xobs, yobs, pch = 21, bg = clrs, cex = cex, cex.axis = cex,
xlab = "", ylab = "")
title(xlab = ii, ylab = "NeedleArea", line = 2)
# fitted, all observations
points(xobs, yfitf, pch = 3, lwd = 2, cex = cex)
# fitted, one omitted observation
points(xobs, yfito, col = clrs2[jj], pch = 6, lwd = 2, cex = cex)
# connect with lines
segments(x0 = xobs, y0 = pmin(yobs, yfitf, yfito),
y1 = pmax(yobs, yfitf, yfito), lty = 2) # connect them
}
legend("bottomright",
legend = c("Observed", "Fitted (all obs.)",
paste0("Fitted (omit ", names(omit.ind)[jj], " obs.)")),
pch = c(21,3,6), lty = c(NA, NA, NA),
lwd = c(NA, 2, 2), pt.bg = "black",
col = c("black", "black", clrs2[jj]),
cex = cex, pt.cex = cex)
}
xobs <- mdata1scl[,ii] # covariate
for(ii in cov1.names) {
# response vs. each covariate
xobs <- mdata1scl[,ii] # covariate
}
ylim <- range(yobs, yfitf, yfito) # y-range of plot
# observed
plot(xobs, yobs, pch = 21, bg = clrs, cex = cex, cex.axis = cex,
xlab = "", ylab = "")
title(xlab = ii, ylab = "NeedleArea", line = 2)
# fitted, all observations
points(xobs, yfitf, pch = 3, lwd = 2, cex = cex)
# fitted, one omitted observation
points(xobs, yfito, col = clrs2[jj], pch = 6, lwd = 2, cex = cex)
# connect with lines
segments(x0 = xobs, y0 = pmin(yobs, yfitf, yfito),
y1 = pmax(yobs, yfitf, yfito), lty = 2) # connect them
cov1.names
cov1.names
clrs2 <- c("red", "blue")
for(jj in 1:length(omit.ind)) {
# model with omitted observation
N1.o <- update(N1, data = mdata1scl-omit.ind[jj])
yfito <- predict(N1.o, newdata = mdata1scl) # fitted values at all observations
length(yfitf)
length(xobs)
for(ii in cov1.names[1:7]) {
# response vs. each covariate
xobs <- mdata1scl[,ii] # covariate
ylim <- range(yobs, yfitf, yfito) # y-range of plot
# observed
plot(xobs, yobs, pch = 21, bg = clrs, cex = cex, cex.axis = cex,
xlab = "", ylab = "")
title(xlab = ii, ylab = "NeedleArea", line = 2)
# fitted, all observations
points(xobs, yfitf, pch = 3, lwd = 2, cex = cex)
# fitted, one omitted observation
points(xobs, yfito, col = clrs2[jj], pch = 6, lwd = 2, cex = cex)
# connect with lines
segments(x0 = xobs, y0 = pmin(yobs, yfitf, yfito),
y1 = pmax(yobs, yfitf, yfito), lty = 2) # connect them
}
legend("bottomright",
legend = c("Observed", "Fitted (all obs.)",
paste0("Fitted (omit ", names(omit.ind)[jj], " obs.)")),
pch = c(21,3,6), lty = c(NA, NA, NA),
lwd = c(NA, 2, 2), pt.bg = "black",
col = c("black", "black", clrs2[jj]),
cex = cex, pt.cex = cex)
}
cov1.names
clrs2 <- c("red", "blue")
for(jj in 1:length(omit.ind)) {
# model with omitted observation
N1.o <- update(N1, data = mdata1scl-omit.ind[jj])
yfito <- predict(N1.o, newdata = mdata1scl) # fitted values at all observations
length(yfitf)
length(xobs)
for(ii in cov1.names[1:7]) {
# response vs. each covariate
xobs <- mdata1scl[,ii] # covariate
ylim <- range(yobs, yfitf, yfito) # y-range of plot
# observed
plot(xobs, yobs, pch = 21, bg = clrs, cex = cex, cex.axis = cex,
xlab = "", ylab = "")
title(xlab = ii, ylab = "NeedleArea", line = 2)
# fitted, all observations
points(xobs, yfitf, pch = 3, lwd = 2, cex = cex)
# fitted, one omitted observation
points(xobs, yfito, col = clrs2[jj], pch = 6, lwd = 2, cex = cex)
# connect with lines
segments(x0 = xobs, y0 = pmin(yobs, yfitf, yfito),
y1 = pmax(yobs, yfitf, yfito), lty = 2) # connect them
}
legend("bottomright",
legend = c("Observed", "Fitted (all obs.)",
paste0("Fitted (omit ", names(omit.ind)[jj], " obs.)")),
pch = c(21,3,6), lty = c(NA, NA, NA),
lwd = c(NA, 2, 2), pt.bg = "black",
col = c("black", "black", clrs2[jj]),
cex = cex, pt.cex = cex)
}
for(jj in 1:length(omit.ind)) {
# model with omitted observation
N1.o <- update(N1, data = mdata1scl-omit.ind[jj])
yfito <- predict(N1.o, newdata = mdata1scl) # fitted values at all observations
length(yfitf)
length(xobs)
for(ii in cov1.names[1:7]) {
# response vs. each covariate
xobs <- mdata1scl[,ii] # covariate
ylim <- range(yobs, yfitf, yfito) # y-range of plot
# observed
plot(xobs, yobs, pch = 21, bg = clrs, cex = cex, cex.axis = cex,
xlab = "", ylab = "")
title(xlab = ii, ylab = "NeedleArea", line = 2)
# fitted, all observations
points(xobs, yfitf, pch = 3, lwd = 2, cex = cex)
# fitted, one omitted observation
points(xobs, yfito, col = clrs2[jj], pch = 6, lwd = 2, cex = cex)
# connect with lines
segments(x0 = xobs, y0 = pmin(yobs, yfitf, yfito),
y1 = pmax(yobs, yfitf, yfito), lty = 2) # connect them
}
legend("bottomright",
legend = c("Observed", "Fitted (all obs.)",
paste0("Fitted (omit ", names(omit.ind)[jj], " obs.)")),
pch = c(21,3,6), lty = c(NA, NA, NA),
lwd = c(NA, 2, 2), pt.bg = "black",
col = c("black", "black", clrs2[jj]),
cex = cex, pt.cex = cex)
}
